#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CluMP Enhanced Implementation with Improved Learning and Prefetching
è«–æ–‡æº–æ‹ CluMPæ”¹è‰¯ç‰ˆ - å­¦ç¿’åŠ¹æœã¨ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒç²¾åº¦ã®å‘ä¸Š

ä¸»è¦æ”¹å–„ç‚¹:
1. MCå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æœ€é©åŒ–
2. ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒæˆ¦ç•¥ã®æ”¹å–„  
3. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã®æœ€é©åŒ–
4. ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ç‰¹æ€§ã«å¿œã˜ãŸé©å¿œçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
"""

from collections import OrderedDict
import logging
import random
import time
import math
from typing import List, Dict, Tuple, Optional, Any, Union

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')


class EnhancedLRUCache:
    """
    å¼·åŒ–ç‰ˆLRUã‚­ãƒ£ãƒƒã‚·ãƒ¥
    
    ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒè¿½è·¡ã¨ãƒ’ãƒƒãƒˆç‡æœ€é©åŒ–ã‚’æ”¹å–„
    """
    
    def __init__(self, cache_size_blocks: int):
        if cache_size_blocks <= 0:
            raise ValueError("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºã¯æ­£ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
            
        self.cache_size = cache_size_blocks
        # (is_prefetched, was_used_after_prefetch, access_count)
        self.cache: OrderedDict[int, Tuple[bool, bool, int]] = OrderedDict()
        
        # è©³ç´°ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒçµ±è¨ˆ
        self.prefetch_stats = {
            "prefetch_total": 0,
            "prefetch_used": 0,
            "prefetch_unused": 0,
            "prefetch_hit": 0,        # ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒãƒ–ãƒ­ãƒƒã‚¯ã¸ã®ç›´æ¥ãƒ’ãƒƒãƒˆ
            "demand_hit": 0,          # é€šå¸¸ã‚¢ã‚¯ã‚»ã‚¹ã§ã®ãƒ’ãƒƒãƒˆ
            "total_accesses": 0
        }
    
    def access(self, block_id: int) -> bool:
        """å¼·åŒ–ç‰ˆãƒ–ãƒ­ãƒƒã‚¯ã‚¢ã‚¯ã‚»ã‚¹å‡¦ç†"""
        self.prefetch_stats["total_accesses"] += 1
        
        if block_id in self.cache:
            is_prefetched, was_used, access_count = self.cache[block_id]
            
            # ã‚¢ã‚¯ã‚»ã‚¹çµ±è¨ˆæ›´æ–°
            if is_prefetched:
                if not was_used:
                    self.prefetch_stats["prefetch_used"] += 1
                    was_used = True
                self.prefetch_stats["prefetch_hit"] += 1
            else:
                self.prefetch_stats["demand_hit"] += 1
            
            # LRUé †åºæ›´æ–°
            self.cache[block_id] = (is_prefetched, was_used, access_count + 1)
            self.cache.move_to_end(block_id)
            
            return True
        
        return False
    
    def insert(self, block_id: int, is_prefetch: bool = False) -> None:
        """å¼·åŒ–ç‰ˆãƒ–ãƒ­ãƒƒã‚¯æŒ¿å…¥"""
        if block_id in self.cache:
            _, was_used, access_count = self.cache[block_id]
            self.cache[block_id] = (is_prefetch, was_used, access_count)
            self.cache.move_to_end(block_id)
            return
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æº€æ¯æ™‚ã®ã‚¨ãƒ“ã‚¯ã‚·ãƒ§ãƒ³
        if len(self.cache) >= self.cache_size:
            lru_block, (was_prefetched, was_used, _) = self.cache.popitem(last=False)
            
            if was_prefetched and not was_used:
                self.prefetch_stats["prefetch_unused"] += 1
        
        # æ–°ãƒ–ãƒ­ãƒƒã‚¯æŒ¿å…¥
        self.cache[block_id] = (is_prefetch, False, 0)
        
        if is_prefetch:
            self.prefetch_stats["prefetch_total"] += 1
    
    def get_hit_rate(self) -> float:
        """ç·åˆãƒ’ãƒƒãƒˆç‡ã‚’è¨ˆç®—"""
        total_hits = self.prefetch_stats["prefetch_hit"] + self.prefetch_stats["demand_hit"]
        total_accesses = self.prefetch_stats["total_accesses"]
        return total_hits / total_accesses if total_accesses > 0 else 0.0
    
    def get_prefetch_efficiency(self) -> float:
        """ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒåŠ¹ç‡ã‚’è¨ˆç®—"""
        total = self.prefetch_stats["prefetch_total"]
        used = self.prefetch_stats["prefetch_used"]
        return used / total if total > 0 else 0.0


class EnhancedMCRow:
    """
    å¼·åŒ–ç‰ˆMCRow - å­¦ç¿’åŠ¹æœã¨äºˆæ¸¬ç²¾åº¦ã‚’æ”¹å–„
    
    æ”¹å–„ç‚¹:
    1. é©å¿œçš„ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
    2. æ™‚é–“é‡ã¿ä»˜ãå­¦ç¿’
    3. äºˆæ¸¬ç²¾åº¦ã®å‹•çš„è©•ä¾¡
    """
    
    def __init__(self):
        # åŸºæœ¬æ§‹é€ ï¼ˆè«–æ–‡æº–æ‹ ï¼‰
        self.CN1: int = -1
        self.CN2: int = -1  
        self.CN3: int = -1
        self.P1: int = 0
        self.P2: int = 0
        self.P3: int = 0
        
        # å¼·åŒ–æ©Ÿèƒ½
        self._last_update_time = {1: 0, 2: 0, 3: 0}
        self._global_time = 0
        self._prediction_history = []  # äºˆæ¸¬æˆåŠŸ/å¤±æ•—å±¥æ­´
        self._confidence_threshold = 3  # äºˆæ¸¬å®Ÿè¡Œã®æœ€å°ä¿¡é ¼åº¦
        
        # é©å¿œçš„å­¦ç¿’ãƒ¬ãƒ¼ãƒˆ
        self._learning_rate = 1.0
        self._decay_factor = 0.99
    
    def update_transition(self, next_chunk_id: int) -> None:
        """å¼·åŒ–ç‰ˆé·ç§»æ›´æ–°"""
        self._global_time += 1
        
        # äºˆæ¸¬ãŒæ­£ã—ã‹ã£ãŸã‹ãƒã‚§ãƒƒã‚¯
        if self.CN1 >= 0:
            prediction_correct = (next_chunk_id == self.CN1)
            self._prediction_history.append(prediction_correct)
            
            # å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™
            if len(self._prediction_history) > 50:
                self._prediction_history.pop(0)
            
            # é©å¿œçš„å­¦ç¿’ãƒ¬ãƒ¼ãƒˆèª¿æ•´
            if prediction_correct:
                self._learning_rate = min(self._learning_rate * 1.05, 2.0)
            else:
                self._learning_rate = max(self._learning_rate * 0.95, 0.5)
        
        # é‡ã¿ä»˜ãé »åº¦æ›´æ–°
        weight = max(1, int(self._learning_rate))
        
        # æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯ã®å ´åˆ
        if next_chunk_id == self.CN1:
            self.P1 += weight
            self._last_update_time[1] = self._global_time
        elif next_chunk_id == self.CN2:
            self.P2 += weight
            self._last_update_time[2] = self._global_time
        elif next_chunk_id == self.CN3:
            self.P3 += weight
            self._last_update_time[3] = self._global_time
        else:
            # æ–°ãƒãƒ£ãƒ³ã‚¯ã®å ´åˆ
            self.CN3 = next_chunk_id
            self.P3 = weight
            self._last_update_time[3] = self._global_time
        
        # æ™‚é–“æ¸›è¡°é©ç”¨
        self._apply_time_decay()
        
        # ã‚½ãƒ¼ãƒˆå®Ÿè¡Œ
        self._sort_candidates()
    
    def _apply_time_decay(self) -> None:
        """æ™‚é–“æ¸›è¡°ã‚’é©ç”¨ã—ã¦å¤ã„é·ç§»ã®å½±éŸ¿ã‚’å‰Šæ¸›"""
        if self._global_time % 100 == 0:  # 100å›ã«1å›å®Ÿè¡Œ
            self.P1 = max(1, int(self.P1 * self._decay_factor)) if self.CN1 >= 0 else 0
            self.P2 = max(1, int(self.P2 * self._decay_factor)) if self.CN2 >= 0 else 0
            self.P3 = max(1, int(self.P3 * self._decay_factor)) if self.CN3 >= 0 else 0
    
    def _sort_candidates(self) -> None:
        """å¼·åŒ–ç‰ˆå€™è£œã‚½ãƒ¼ãƒˆ"""
        candidates = []
        
        if self.CN1 >= 0:
            candidates.append((self.CN1, self.P1, self._last_update_time[1], 1))
        if self.CN2 >= 0:
            candidates.append((self.CN2, self.P2, self._last_update_time[2], 2))
        if self.CN3 >= 0:
            candidates.append((self.CN3, self.P3, self._last_update_time[3], 3))
        
        # ã‚½ãƒ¼ãƒˆï¼šé »åº¦é™é †ã€åŒå€¤ãªã‚‰æ›´æ–°æ™‚åˆ»é™é †
        candidates.sort(key=lambda x: (-x[1], -x[2]))
        
        # ãƒªã‚»ãƒƒãƒˆ
        self.CN1 = self.CN2 = self.CN3 = -1
        self.P1 = self.P2 = self.P3 = 0
        
        # å†å‰²ã‚Šå½“ã¦
        for i, (chunk_id, freq, update_time, _) in enumerate(candidates):
            if i == 0:
                self.CN1, self.P1 = chunk_id, freq
                self._last_update_time[1] = update_time
            elif i == 1:
                self.CN2, self.P2 = chunk_id, freq
                self._last_update_time[2] = update_time
            elif i == 2:
                self.CN3, self.P3 = chunk_id, freq
                self._last_update_time[3] = update_time
    
    def predict_next_chunk(self) -> Optional[int]:
        """ä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹äºˆæ¸¬"""
        if self.CN1 < 0 or self.P1 < self._confidence_threshold:
            return None
        
        # äºˆæ¸¬ç²¾åº¦ãŒä½ã„å ´åˆã¯äºˆæ¸¬ã‚’æ§ãˆã‚‹
        if len(self._prediction_history) > 10:
            recent_accuracy = sum(self._prediction_history[-10:]) / 10
            if recent_accuracy < 0.3:
                return None
        
        return self.CN1
    
    def get_prediction_confidence(self) -> float:
        """äºˆæ¸¬ä¿¡é ¼åº¦ã‚’å–å¾—"""
        if self.CN1 < 0:
            return 0.0
        
        total_freq = self.P1 + self.P2 + self.P3
        if total_freq == 0:
            return 0.0
        
        # é »åº¦ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
        freq_confidence = self.P1 / total_freq
        
        # å±¥æ­´ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
        hist_confidence = 0.5
        if len(self._prediction_history) > 5:
            hist_confidence = sum(self._prediction_history[-10:]) / min(10, len(self._prediction_history))
        
        return (freq_confidence + hist_confidence) / 2


class EnhancedCluMPSimulator:
    """
    å¼·åŒ–ç‰ˆCluMPã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿
    
    æ”¹å–„ç‚¹:
    1. é©å¿œçš„ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒçª“ã‚µã‚¤ã‚º
    2. ä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒ
    3. å­¦ç¿’æœŸé–“ã®è€ƒæ…®
    4. ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒç¯„å›²ã®æœ€é©åŒ–
    """
    
    def __init__(self, chunk_size_blocks: int, cluster_size_chunks: int, 
                 cache_size_blocks: int, initial_prefetch_window: int = 16):
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¤œè¨¼
        if any(x <= 0 for x in [chunk_size_blocks, cluster_size_chunks, 
                                cache_size_blocks, initial_prefetch_window]):
            raise ValueError("ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯æ­£ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        
        self.chunk_size = chunk_size_blocks
        self.cluster_size = cluster_size_chunks
        self.cache_size = cache_size_blocks
        self.initial_prefetch_window = initial_prefetch_window
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.cache = EnhancedLRUCache(cache_size_blocks)
        self.clusters: Dict[int, Dict[int, EnhancedMCRow]] = {}
        
        # é©å¿œçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.current_prefetch_window = initial_prefetch_window
        self.min_prefetch_window = 4
        self.max_prefetch_window = 64
        
        # çµ±è¨ˆ
        self.total_accesses = 0
        self.cache_hits = 0
        self.previous_chunk_id: Optional[int] = None
        self.successful_predictions = 0
        self.total_predictions = 0
        
        # å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºåˆ¶å¾¡
        self.learning_phase_length = 1000  # æœ€åˆã®1000ã‚¢ã‚¯ã‚»ã‚¹ã¯å­¦ç¿’é‡è¦–
        self.adaptive_prefetch_enabled = False
        
        logging.info(f"å¼·åŒ–ç‰ˆCluMPã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿åˆæœŸåŒ–: chunk={chunk_size_blocks}, "
                    f"cluster={cluster_size_chunks}, cache={cache_size_blocks}")
    
    def _get_chunk_id(self, block_id: int) -> int:
        """ãƒ–ãƒ­ãƒƒã‚¯IDã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯IDã‚’è¨ˆç®—"""
        return block_id // self.chunk_size
    
    def _get_mc_row(self, chunk_id: int, allocate: bool = False) -> Optional[EnhancedMCRow]:
        """MCRowã‚’å–å¾—ï¼ˆå‹•çš„å‰²ã‚Šå½“ã¦ï¼‰"""
        cluster_id = chunk_id // self.cluster_size
        chunk_in_cluster = chunk_id % self.cluster_size
        
        if cluster_id not in self.clusters:
            if not allocate:
                return None
            self.clusters[cluster_id] = {}
        
        if chunk_in_cluster not in self.clusters[cluster_id]:
            if not allocate:
                return None
            self.clusters[cluster_id][chunk_in_cluster] = EnhancedMCRow()
        
        return self.clusters[cluster_id][chunk_in_cluster]
    
    def _adaptive_prefetch_chunk(self, chunk_id: int, confidence: float) -> None:
        """é©å¿œçš„ãƒãƒ£ãƒ³ã‚¯ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒ"""
        start_block = chunk_id * self.chunk_size
        
        # ä¿¡é ¼åº¦ã«åŸºã¥ãçª“ã‚µã‚¤ã‚ºèª¿æ•´
        confidence_factor = min(confidence * 2, 1.0)
        effective_window = max(self.min_prefetch_window, 
                              int(self.current_prefetch_window * confidence_factor))
        
        prefetch_count = 0
        for i in range(effective_window):
            prefetch_block = start_block + i
            if not self.cache.access(prefetch_block):
                self.cache.insert(prefetch_block, is_prefetch=True)
                prefetch_count += 1
                
                # é©å¿œçš„ãªç¯„å›²åˆ¶é™
                if prefetch_count >= self.max_prefetch_window:
                    break
    
    def _update_prefetch_window(self, prediction_success: bool) -> None:
        """ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒçª“ã‚µã‚¤ã‚ºã®é©å¿œçš„èª¿æ•´"""
        if not self.adaptive_prefetch_enabled:
            return
        
        if prediction_success:
            # æˆåŠŸæ™‚ï¼šçª“ã‚µã‚¤ã‚ºã‚’å°‘ã—å¢—åŠ 
            self.current_prefetch_window = min(
                self.current_prefetch_window * 1.1,
                self.max_prefetch_window
            )
        else:
            # å¤±æ•—æ™‚ï¼šçª“ã‚µã‚¤ã‚ºã‚’å°‘ã—æ¸›å°‘  
            self.current_prefetch_window = max(
                self.current_prefetch_window * 0.9,
                self.min_prefetch_window
            )
    
    def process_access(self, block_id: int) -> bool:
        """å¼·åŒ–ç‰ˆãƒ–ãƒ­ãƒƒã‚¯ã‚¢ã‚¯ã‚»ã‚¹å‡¦ç†"""
        self.total_accesses += 1
        current_chunk_id = self._get_chunk_id(block_id)
        
        # å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºå®Œäº†ãƒã‚§ãƒƒã‚¯
        if self.total_accesses == self.learning_phase_length:
            self.adaptive_prefetch_enabled = True
            logging.info("å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºå®Œäº†ã€é©å¿œçš„ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒæœ‰åŠ¹åŒ–")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¢ã‚¯ã‚»ã‚¹
        cache_hit = self.cache.access(block_id)
        if cache_hit:
            self.cache_hits += 1
        else:
            self.cache.insert(block_id, is_prefetch=False)
        
        # MCæ›´æ–°ã¨äºˆæ¸¬
        prediction_made = False
        prediction_success = False
        
        if self.previous_chunk_id is not None:
            # MCæ›´æ–°
            mc_row = self._get_mc_row(self.previous_chunk_id, allocate=True)
            mc_row.update_transition(current_chunk_id)
            
            # ç¾åœ¨ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ã®äºˆæ¸¬å®Ÿè¡Œ
            current_mc_row = self._get_mc_row(current_chunk_id, allocate=False)
            if current_mc_row is not None:
                predicted_chunk = current_mc_row.predict_next_chunk()
                if predicted_chunk is not None:
                    confidence = current_mc_row.get_prediction_confidence()
                    
                    # ä¿¡é ¼åº¦ãŒååˆ†é«˜ã„å ´åˆã®ã¿ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒ
                    if confidence > 0.4:  # é–¾å€¤èª¿æ•´
                        self._adaptive_prefetch_chunk(predicted_chunk, confidence)
                        prediction_made = True
                        self.total_predictions += 1
        
        # ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒçª“ã‚µã‚¤ã‚ºèª¿æ•´
        if prediction_made:
            self._update_prefetch_window(prediction_success)
        
        self.previous_chunk_id = current_chunk_id
        return cache_hit
    
    def get_evaluation_metrics(self) -> Dict[str, Any]:
        """å¼·åŒ–ç‰ˆè©•ä¾¡æŒ‡æ¨™"""
        hit_rate = self.cache.get_hit_rate()
        prefetch_efficiency = self.cache.get_prefetch_efficiency()
        
        # MCçµ±è¨ˆè¨ˆç®—
        total_mc_rows = sum(len(cluster) for cluster in self.clusters.values())
        memory_usage = total_mc_rows * 24  # 24B per MCRow
        
        # äºˆæ¸¬ç²¾åº¦è¨ˆç®—
        prediction_accuracy = 0.0
        if self.total_predictions > 0:
            prediction_accuracy = self.successful_predictions / self.total_predictions
        
        return {
            "total_accesses": self.total_accesses,
            "cache_hits": self.cache_hits,
            "hit_rate": hit_rate,
            "prefetch_total": self.cache.prefetch_stats["prefetch_total"],
            "prefetch_used": self.cache.prefetch_stats["prefetch_used"],
            "prefetch_unused": self.cache.prefetch_stats["prefetch_unused"],
            "prefetch_efficiency": prefetch_efficiency,
            "memory_usage_mc_rows": total_mc_rows,
            "memory_usage_bytes": memory_usage,
            "memory_usage_kb": memory_usage / 1024,
            "prediction_accuracy": prediction_accuracy,
            "current_prefetch_window": self.current_prefetch_window,
            "adaptive_enabled": self.adaptive_prefetch_enabled,
            "chunk_size": self.chunk_size,
            "cluster_size": self.cluster_size,
            "cache_size": self.cache_size,
        }


def enhanced_comparison_test():
    """å¼·åŒ–ç‰ˆæ¯”è¼ƒãƒ†ã‚¹ãƒˆ"""
    print("ğŸš€ CluMPå¼·åŒ–ç‰ˆæ€§èƒ½ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    # æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    enhanced_params = {
        "chunk_size": 8,
        "cluster_size": 32,
        "initial_prefetch_window": 12
    }
    
    cache_size = 4096
    
    # ã‚ˆã‚Šç¾å®Ÿçš„ãªãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ç”Ÿæˆ
    def generate_enhanced_kvm_workload(total_blocks: int = 12000) -> List[int]:
        trace = []
        # Phase 1: ãƒ–ãƒ¼ãƒˆé€æ¬¡èª­ã¿è¾¼ã¿ (é«˜ã„å±€æ‰€æ€§)
        base = random.randint(0, 10000)
        for i in range(total_blocks // 3):
            if random.random() < 0.9:  # 90% é€æ¬¡
                trace.append(base + i)
            else:
                base += random.randint(1, 10)
                trace.append(base)
        
        # Phase 2: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ (ä¸­ç¨‹åº¦ã®å±€æ‰€æ€§)
        for _ in range(total_blocks // 3):
            if random.random() < 0.6:  # 60% å±€æ‰€çš„
                base += random.randint(1, 50)
                for j in range(random.randint(3, 15)):
                    trace.append(base + j)
            else:
                trace.append(random.randint(0, 100000))
        
        # Phase 3: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èª­ã¿è¾¼ã¿ (ä½ã„å±€æ‰€æ€§)
        for _ in range(total_blocks - len(trace)):
            pattern = random.random()
            if pattern < 0.4:
                trace.append(base + random.randint(1, 100))
            elif pattern < 0.7:
                base += random.randint(100, 1000)
                trace.append(base)
            else:
                trace.append(random.randint(0, 200000))
        
        return trace
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    trace = generate_enhanced_kvm_workload()
    
    # å¼·åŒ–ç‰ˆCluMP
    enhanced_clump = EnhancedCluMPSimulator(
        chunk_size_blocks=enhanced_params["chunk_size"],
        cluster_size_chunks=enhanced_params["cluster_size"],
        cache_size_blocks=cache_size,
        initial_prefetch_window=enhanced_params["initial_prefetch_window"]
    )
    
    for block_id in trace:
        enhanced_clump.process_access(block_id)
    
    enhanced_results = enhanced_clump.get_evaluation_metrics()
    
    # Linuxå…ˆèª­ã¿ï¼ˆæ¯”è¼ƒç”¨ï¼‰
    from clump_simulator import LinuxReadAhead
    readahead = LinuxReadAhead(cache_size_blocks=cache_size)
    
    for block_id in trace:
        readahead.process_access(block_id)
    
    readahead_results = readahead.get_evaluation_metrics()
    
    # çµæœè¡¨ç¤º
    print(f"ğŸ“Š çµæœæ¯”è¼ƒ")
    print("-" * 30)
    print(f"Linuxå…ˆèª­ã¿:")
    print(f"  ãƒ’ãƒƒãƒˆç‡: {readahead_results['hit_rate']:.3f}")
    print(f"  ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒåŠ¹ç‡: {readahead_results['prefetch_efficiency']:.3f}")
    
    print(f"CluMPå¼·åŒ–ç‰ˆ:")
    print(f"  ãƒ’ãƒƒãƒˆç‡: {enhanced_results['hit_rate']:.3f}")
    print(f"  ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒåŠ¹ç‡: {enhanced_results['prefetch_efficiency']:.3f}")
    print(f"  MCè¡Œæ•°: {enhanced_results['memory_usage_mc_rows']}")
    print(f"  äºˆæ¸¬ç²¾åº¦: {enhanced_results['prediction_accuracy']:.3f}")
    print(f"  æœ€çµ‚çª“ã‚µã‚¤ã‚º: {enhanced_results['current_prefetch_window']:.1f}")
    
    improvement = enhanced_results['hit_rate'] / readahead_results['hit_rate'] if readahead_results['hit_rate'] > 0 else 0
    print(f"\næ”¹å–„å€ç‡: {improvement:.2f}x")
    
    return enhanced_results, readahead_results


if __name__ == "__main__":
    random.seed(42)
    enhanced_comparison_test()